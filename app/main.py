import os
import sys
import torch
from fastapi import FastAPI, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import io

# ëª¨ë¸ê³¼ ì¶”ë¡  ê´€ë ¨ í•¨ìˆ˜ ì„í¬íŠ¸
from app.model.model import MultiTaskMobileNetV3  # ëª¨ë¸ ì •ì˜
from app.model.inference import data_transforms, predict_image  # ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡ í•¨ìˆ˜

# âš ï¸ torch.load ì˜¤ë¥˜ ë°©ì§€: í´ë˜ìŠ¤ ê²½ë¡œ ë“±ë¡
sys.modules["__main__"].MultiTaskMobileNetV3 = MultiTaskMobileNetV3

app = FastAPI()

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë””ë°”ì´ìŠ¤ ì„¤ì • (ë Œë”ëŠ” GPU ë¯¸ì§€ì› â†’ CPU)
device = torch.device("cpu")

# ëª¨ë¸ ê²½ë¡œ
model_path = os.path.join(os.path.dirname(__file__), "model", "MTL_BASIS.pth")
print(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")

# ëª¨ë¸ ë¡œë“œ
try:
    model = torch.load(model_path, map_location=device, weights_only=False)
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    raise Exception("ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ìš© ë¼ìš°íŠ¸
@app.get("/")
def read_root():
    return {"message": "ğŸ§  MTL AI API is running!"}

# ì˜ˆì¸¡ ë¼ìš°íŠ¸
@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    try:
        contents = await file.read()
        results = predict_image(model, contents, device=device)
        return {"results": results}
    except Exception as e:
        return {"error": f"An error occurred during prediction: {str(e)}"}
