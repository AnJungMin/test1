import os
import sys
import torch
from fastapi import FastAPI, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import io

# ëª¨ë¸ê³¼ ì¶”ë¡  ê´€ë ¨ í•¨ìˆ˜ ì„í¬íŠ¸
from app.model.model import MultiTaskMobileNetV3  # ëª¨ë¸ ì •ì˜ íŒŒì¼
from app.model.inference import data_transforms, predict_image  # ì¶”ë¡  í•¨ìˆ˜

# __main__ ëª¨ë“ˆì— ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ë“±ë¡ (ëª¨ë¸ ì €ì¥ ì‹œ __main__ì— ì €ì¥ëœ ê²½ìš° ëŒ€ë¹„)
sys.modules["__main__"].MultiTaskMobileNetV3 = MultiTaskMobileNetV3

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ëª¨ë¸ ë¡œë“œ (GPU ëŒ€ì‹  CPU ì‚¬ìš©)
device = torch.device("cpu")

# ëª¨ë¸ ê²½ë¡œ ì„¤ì • (í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ)
model_path = os.path.join(os.path.dirname(__file__), "model", "MTL_BASIS.pth")
print(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")

model = None
try:
    from torch.serialization import safe_globals
    # ì‹¤ì œ í´ë˜ìŠ¤ ê°ì²´ë¥¼ ì „ë‹¬í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³ , weights_only ì˜µì…˜ì„ Falseë¡œ ì„¤ì •
    with safe_globals([MultiTaskMobileNetV3]):
        model = torch.load(model_path, map_location=device, weights_only=False)
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    print("ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    raise Exception("ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

@app.get("/")
def read_root():
    return {"message": "ğŸ§  MTL AI API is running!"}

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    try:
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert("RGB")
        image = data_transforms(image)  # ì´ë¯¸ì§€ ì „ì²˜ë¦¬

        # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡
        results = predict_image(model, image, device=device)

        return {"results": results}

    except Exception as e:
        return {"error": f"An error occurred during prediction: {str(e)}"}
