from fastapi import FastAPI, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import io
import torch
import os

# ğŸ”½ your_modelê³¼ inference ë¶ˆëŸ¬ì˜¤ê¸°
from app.model.your_model import MultiTaskMobileNetV3
from app.model.inference import data_transforms, predict_image

# ========================
# FastAPI ì•± ì´ˆê¸°í™”
# ========================
app = FastAPI()

# CORS ì„¤ì • (í•„ìš” ì‹œ ë„ë©”ì¸ ì œí•œ ê°€ëŠ¥)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========================
# ëª¨ë¸ ë¡œë“œ
# ========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì ˆëŒ€ ê²½ë¡œë¡œ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
model_path = os.path.join(os.path.dirname(__file__), "app", "model", "MTL_BASIS.pth")

# ëª¨ë¸ ë¡œë“œ
model = None
try:
    from torch.serialization import safe_globals
    with safe_globals(["MultiTaskMobileNetV3"]):  # ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ê¸€ë¡œë²Œë¡œ ì¶”ê°€
        model = torch.load(model_path, map_location=device)
        model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
except Exception as e:
    print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    # ì•±ì´ ì¢…ë£Œë˜ì§€ ì•Šë„ë¡ ì˜ˆì™¸ë¥¼ ë˜ì§€ê¸°
    raise Exception("ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# ========================
# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸
# ========================
@app.get("/")
def read_root():
    return {"message": "ğŸ§  MTL AI API is running!"}

# ========================
# ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸
# ========================
@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    try:
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert("RGB")
        image = data_transforms(image)  # ì´ë¯¸ì§€ë¥¼ ë³€í™˜

        # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡
        results = predict_image(model, image, device=device)

        return {"results": results}

    except Exception as e:
        return {"error": f"An error occurred during prediction: {str(e)}"}
